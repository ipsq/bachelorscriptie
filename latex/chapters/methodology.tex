To show the positive impact big data has had on our ability to correctly model the term structure of interest rates, we will be comparing small data models with big data models. 
The models and predictive accuracy tests will be based on those presented in~\textcite{Swanson2017}. 
The small data models will include autoregressive, vector autoregressive, and dynamic Nelson-Siegel models. 
The big data models will include models utilizing diffusion indexes estimated from a macroeconomic dataset. 
The models will be (re-)estimated prior to construction of each forecast, using a rolling window of 120 months. 
The use of rolling windows is explained in~\cref{sec:dmtest}. 
Estimation of the models is done using least squares and, where necessary, (sparse) principal component analysis. 
Monthly yield forecasts for the horizons $h = 1-$, $3-$, and $12-$steps ahead are constructed for the bond maturities $\tau = 1-$, $2-$, $3-$, $5-$, and $10-$years. 
Performance of these models shall be assessed by mean square forecast error (MSFE) and models shall be compared for predictive accuracy using~\textcite[hereafter DM]{Diebold1994}. 

\section{Model selection}
\subsection{Cross validation}


\subsection{Mean Squared Prediction Error (MSPE)}


\subsection{Schwarz information criterion (SIC)}
\label{sec:sic}
The Schwarz information criterion, also commonly referred to as the Bayesian information criterion, published by~\textcite[hereafter SIC]{Schwarz1978} is a model selection criterium, closely related to the Akaike information criterion~\parencite{Akaike1974}. 
It is defined as follows
\begin{equation}
	SIC = k\ln{n} - 2\ln{\hat{L}}
\end{equation}
where $\hat{L}$ is the likelihood of the model, $n$ the number of observations and $k$ the number of model parameters estimated. The difference between SIC and AIC is the higher penalty given for the addition of extra parameters to compensate for the increase in likelihood.

\section{Dimension reduction}
\subsection{Principal Component Analysis (PCA)}
Principal Component Analysis is a method of finding a variance maximizing orthogonal basis for your data in which different dimensions are uncorrelated. Principal Component Analysis is most often achieved in one of two ways:
\begin{enumerate}
	\item singular value decomposition
	\item eigenvalue decomposition on the covariance matrix
\end{enumerate}


\subsection{Sparse Principal Component Analysis (SPCA)}
\label{sec:spca}
\textcite{Tikhonov978} introduced the ridge regression.
The ridge regression is a regularization method to mitigate the problem of multicollinearity in linear regression.
It imposes a penalty on the squared value of the regression coefficients, effectively shrinking them towards zero.
\begin{equation}
	\beta_{ridge} = \argmin_{\beta} \left\{ { \left\lVert Y - \sum_{j=1}^p X_j \beta_j \right\rVert }^2 + \lambda \sum_{j=1}^p \beta_j^2 \right\} 
\end{equation}
\textcite{Tibshirani1996} later introduced the \textit{least absolute shrinkage and selection operator} (lasso) regression. 
This method imposes a penalty on the absolute value of the regression coefficients, effectively eliminating some coefficients from the model.
\begin{equation}
	\beta_{lasso} = \argmin_{\beta} \left\{ { \left\lVert Y - \sum_{j=1}^p X_j \beta_j \right\rVert }^2 + \lambda \sum_{j=1}^p |\beta_j| \right\} 
\end{equation}
Combining these two regularization methods resulted in what is now known as the elastic net, see~\textcite{Zou2005}.
\begin{equation}
	\beta_{elastic\ net} = (1 + \lambda_2) \argmin_{\beta} \left\{ { \left\lVert Y - \sum_{j=1}^p X_j \beta_j \right\rVert }^2 + \lambda_2 \sum_{j=1}^p \beta_j^2 + \lambda_1 \sum_{j=1}^p |\beta_j|  \right\} 
\end{equation}
Factors estimated with Principal Component Analysis create linear combinations of the underlying predictor variables with all non-zero factor loadings. 
This negatively affects the parsimony of forecasting models and makes interpreting the factor loadings difficult. 
\textcite{Zou2006} introduced a method of finding principal components with sparse factor loadings by formulating PCA as a regression problem and applying the elastic net. 
\begin{equation}
\begin{aligned}
	\left(\mathbf{\widehat{A}},\mathbf{\widehat{B}}\right) = \argmin_{\mathbf{A},\mathbf{B}}\ &\sum_{i=1}^n { \left\lVert \mathbf{x}_i - \mathbf{A}\mathbf{B}^T\mathbf{x}_i \right\rVert }^2 + \lambda_2 \sum_{j=1}^k {\lVert \beta_j \rVert}^2 + \sum_{j=1}^k \lambda_{1,j} {\lVert \beta_j \rVert}_1 \\
	\text{subject to}\ &\mathbf{A}^T\mathbf{A} = \mathbf{I}_{k \times k}
\end{aligned}
\end{equation}

\section{Models}
\subsection{Autoregressive (AR) and Vector-Autoregressive (VAR) Models}
\label{sec:arvar}
These AR($p$) and VAR($p$) models are formulated as follows:
\begin{equation}
	y_{t+1}(\tau) = c + \beta' W_t + \epsilon_{t+1},
\end{equation}
where $\tau$ denotes the maturity and $y_{t+1}(\tau)$ measures the 1-step ahead annual yield forecast of the bond. 
For the autoregressive model, $W_t$ contains $p$ lags of $y_{t+1}(\tau)$. For the vector autoregressive model, $W_t$ additionally contains yields of bonds of different maturities than $\tau$. 
In both models $\beta$ is a time-invariant coefficient vector and $c$ is a (vector of) constant(s). 
We will be including AR(1) and VAR(1) models as a baseline. 
Furthermore, we will be including AR and VAR specifications with up to 5 lags of $y_{t+1}(\tau)$ included. 
The number of lags shall be selected using the Schwarz information criterion (SIC), see~\cref{sec:sic}. 
These models shall be referenced by AR(SIC) and VAR(SIC). 

\subsection{Dynamic Nelson-Siegel (DNS) Models}
\label{sec:dns}
The Nelson-Siegel model, published by~\textcite{Nelson1987}, models the cross-sectional movement of the term structure using three underlying latent factors interpreted as \enquote{level}, \enquote{slope}, and \enquote{curvature}. 
These factors are commonly known as the \enquote{Nelson-Siegel factors}. 
The dynamic version of the Nelson-Siegel model was published by~\textcite[hereafter DNS]{Diebold2006}. 
The DNS model is formulated as follows:
\begin{equation}
	y_{t+1}(\tau) = \hat{\beta}_{1,t+1}^{f} + \hat{\beta}_{2,t+1}^{f} \left[\frac{1-\exp(-\lambda_t \tau)}{\lambda_t \tau}\right] + \hat{\beta}_{3,t+1}^{f} \left[\frac{1-\exp(-\lambda_t \tau)}{\lambda_t \tau} - \exp(-\lambda_t \tau)\right],
\end{equation}
where $\hat{\beta}_{1,t+1}^{f}$, $\hat{\beta}_{2,t+1}^{f}$, and $\hat{\beta}_{3,t+1}^{f}$ are estimated using an AR(1) model:
\begin{equation}
\label{eq:dnsar}
	\hat{\beta}_{i,t+1}^{f} = \hat{c}_i + \hat{\gamma}_{ii} \hat{\beta}_{i,t}^f, \quad \text{for}~i = 1,2,3,
\end{equation}
where $\hat{c}_i$, $\hat{\gamma}_{ii}$, and $\hat{\beta}_{i,t}^f$ are scalars. 
Alternatively, $\hat{\beta}_{1,t+1}^{f}$, $\hat{\beta}_{2,t+1}^{f}$, and $\hat{\beta}_{3,t+1}^{f}$ can also be estimated using a VAR(1) model:
\begin{equation}
\label{eq:dnsvar}
	\hat{\beta}_{t+1}^{f} = \hat{c} + \hat{\Gamma} \hat{\beta}_t^f,
\end{equation}
where $\hat{c}$ is a $3 \times 1$ vector, $\hat{\beta}_t^f = \left(\hat{\beta}_{1,t}^f, \hat{\beta}_{2,t}^f, \hat{\beta}_{3,t}^f\right)'$ a $3 \times 1$ vector, and $\hat{\Gamma} = \left(\hat{\gamma}_1, \hat{\gamma}_2, \hat{\gamma}_3 \right)$, with $\hat{\gamma}_j$ a $3 \times 1$ vector, for $j = 1,2,3$. 
The parameters $\hat{\beta}_{1,t}^f, \hat{\beta}_{2,t}^f, \hat{\beta}_{3,t}^f$ are obtained by regressing $\left\{1, \left[\frac{1-\exp(-\lambda_t \tau)}{\lambda_t \tau}\right], \left[\frac{1-\exp(-\lambda_t \tau)}{\lambda_t \tau} - \exp(-\lambda_t \tau)\right] \right\}$ on $\mathbf{y}_t^f(\tau)$. 
We will consider 3 variants of $\mathbf{y}_t^f(\tau)$ in this paper, namely:
\begin{align*}
	\mathbf{y}_t^{10}(\tau) &= \left[y_t(12)~y_t(24)~y_t(36)~y_t(48)~y_t(60)~y_t(72)~y_t(84)~y_t(96)~y_t(108)~y_t(120) \right]', \\
	\mathbf{y}_t^6(\tau) &= \left[y_t(12)~y_t(24)~y_t(36)~y_t(60)~y_t(84)~y_t(120) \right]', \\
	\mathbf{y}_t^4(\tau) &= \left[y_t(12)~y_t(36)~y_t(60)~y_t(120) \right]'.
\end{align*}
As discussed in \textcite{Diebold2006}, we will be setting the rate of decay, $\lambda_t$, to $0.0609$.

\subsubsection{DNS Models with Macroeconomic Variables}
\label{sec:dnsmv}
The DNS model, described in~\cref{sec:dns}, can be extended by including macroeconomic variables in the AR(1) model, given in~\cref{eq:dnsar}, as follows:
\begin{equation}
	\hat{\beta}_{i,t+1}^{f} = \hat{c}_i + \hat{\gamma}_{ii} \hat{\beta}_{i,t}^f + \hat{\alpha}_i' M_t, \quad \text{for}~i = 1,2,3,
\end{equation}
where $\hat{\alpha}_i$ is a $3 \times 1$ vector, and $M_t$ is a $3 \times 1$ vector containing the macroeconomic variables \enquote{manufacturing capacity utilization}, \enquote{the federal funds rate}, and \enquote{the annual personal consumption expenditures price deflator}. 
All other terms are conformably defined, see~\cref{sec:dns}. 
Analogous to the AR(1) model, the VAR(1) given in~\cref{eq:dnsvar}, can also be extended by including macroeconomic variables:
\begin{equation}
	\hat{\beta}_{t+1}^{f} = \hat{c} + \hat{\Gamma} \hat{\beta}_t^f + \hat{\mathrm{A}} M_t,
\end{equation}
where $\hat{\mathrm{A}} = \left(\hat{\alpha}_1, \hat{\alpha}_2, \hat{\alpha}_3 \right)$, with $\hat{\alpha}_j$ a $3 \times 1$ vector, for $j = 1,2,3$. 
All other terms are conformably defined, see~\cref{sec:dns}.

\subsubsection{DNS Models with Diffusion Indexes}
\label{sec:dnsdif}
The DNS model given in~\cref{sec:dns}, can also be extended by including diffusion indexes. The extended version of the AR model is formulated as follows:
\begin{equation}
	\hat{\beta}_{i,t+1}^{f} = \hat{c}_i + \hat{\gamma}_{ii} \hat{\beta}_{i,t}^f + \hat{\xi}' F_t^b, \quad \text{for}~i = 1,2,3,
\end{equation}
where $F_t^b$ is defined as in~\cref{sec:dif} and $\hat{\xi}$ is a conformably sized (depending on the amount of factors included) vector of coefficients. 
All other terms are conformably defined, see~\cref{sec:dns}. 
Analogous to the AR(1) model, the VAR(1) given in~\cref{eq:dnsvar}, can also be extended by including diffusion indexes:
\begin{equation}
	\hat{\beta}_{t+1}^{f} = \hat{c} + \hat{\Gamma} \hat{\beta}_t^f + \hat{\Xi} F_t^b,
\end{equation}
where $\hat{\Xi}$ is a conformably sized (depending on the amount of factors included) matrix of coefficients. 
All other terms are conformably defined, see~\cref{sec:dns}.

\subsection{Diffusion Index Models}
\label{sec:dif}
Diffusion Index Models, also commonly referred to as \enquote{factor augmented forecast models}, as published by~\textcite[hereafter DIF]{Stock2002a,Stock2002b}, is an extension to regular forecast models, mostly (vector-)autoregressive models, supplemented by a vector of latent factors. 
This model is formulated as follows:
\begin{equation}
	y_{t+1}(\tau) = c + \beta' W_t + \xi' F_t^b + \eta' F_t^s + \epsilon_{t+1},
\end{equation}
where $F_t^b$ contains either 1, 2, or 3 latent factors, estimated from the set of macroeconomic variables, using either PCA or SPCA (see~\cref{sec:spca}), $F_t^s$ contains either 1, 2, or 3 latent factors, estimated with PCA, from a set of 10 yields given by $\mathbf{y}_t^{10}(\tau)$, see~\cref{sec:dns}, and $\xi$ and $\eta$ are conformably sized (depending on the amount of factors included) vectors of coefficients. 

\section{Predictive accuracy testing}
\subsection{Diebold-Mariano (DM) Test}
\label{sec:dmtest}
The Diebold-Mariano Test, published by~\textcite[hereafter DM]{Diebold1994}, is a predictive accuracy test between two models. The hypotheses are defined as follows:
\begin{align*}
	H_0 : E\left[L(u_{0,t+h}) - L(u_{1,t+h})\right] = 0, \\
	H_A : E\left[L(u_{0,t+h}) - L(u_{1,t+h})\right] \neq 0,
\end{align*}
where $L(\cdot)$ is a loss function, and $u_{0,t+h}$ and $u_{1,t+h}$ are $h$-step ahead forecast errors. In practice, we only observe estimates of these out-of-sample $h$-step ahead forecasts, i.e. $\hat{u}_{0,t+h}$ and $\hat{u}_{1,t+h}$. The standard version of the DM predictive accuracy test is formulated as follows:
\begin{equation}
	DM_p = \frac{\bar{d_t}}{\hat{\sigma}_{\bar{d}_t}} \overset{d}{\rightarrow} N(0,1),
\end{equation}
where
\begin{equation}
	\bar{d}_t = \frac{1}{P} \sum_{t=R+1}^{T} d_t,\ d_t = L(\hat{u}_{0,t+h}) - L(\hat{u}_{1,t+h}),\ \text{and}\ \hat{\sigma}_{\bar{d}_t} = \frac{\hat{\sigma}_{d_t}}{\sqrt{P}}.
\end{equation}
In the formulation above, $P$ is defined as the number of forecasts, $R$ is defined as the number of observations in the training sample, and $T = P + R$. 
