To show the positive impact big data has had on our ability to correctly model the term structure of interest rates, we are comparing small data models with big data models. 
The models and predictive accuracy tests are based on those presented in~\textcite{swanson_big_2017}. 
The small data models are autoregressive, vector autoregressive, and dynamic Nelson-Siegel models. 
The big data models are diffusion index models estimated from a macroeconomic dataset, using dimension reduction techniques described in~\cref{sec:dimension-reduction}. 
Descriptive overview of the models used is given in~\cref{tab:model-description}.
Detailed explanation of the models used is given in~\cref{sec:models}.
The models are (re-)estimated prior to construction of each forecast, using a rolling window of 120 months.
Estimation of the models is done using maximum likelihood estimation and, where necessary, (sparse) principal component analysis. 
Monthly yield $h = 1-$step ahead forecasts are constructed for the bond maturities $\tau = 1-$, $2-$, $3-$, $5-$, and $10-$years. 
Performance of these models is assessed by mean square prediction error and models are compared for predictive accuracy, against the benchmark autoregressive model with a lag order of 1, using~\textcite{diebold_comparing_1994}. 
We are also comparing the predictive accuracy of various variations of dynamic Nelson-Siegel models, diffusion index models, and we are comparing principal component analysis augmented models to sparse principal component analysis augmented models, using the test presented by~\citeauthor{diebold_comparing_1994}. 

\section{Dimension reduction}
\label{sec:dimension-reduction}
\subsection{Principal Component Analysis (PCA)}
Principal Component Analysis is a method of finding an orthogonal basis for your data in which different dimensions are uncorrelated. 
The vectors spanning this orthogonal basis are sorted beginning with the most variance explained to least variance explained. 
This method is often used as a dimension reduction technique by only including the first $k$ factors to explain the data, whilst keeping most variance of the original dataset. 
The PCA coefficients are most often computed in one of two ways: eigenvalue decomposition on the correlation matrix, or singular value decomposition.
In this paper, we only focus on singular value decomposition.
Both methods result in the same set of principal components but the eigenvalue decomposition method is more restricted than the singular value decomposition. 
This is (part of) the reason why singular value decomposition is used for the sparse principal component analysis formulation.

Let $\mathbf{X} = \begin{pmatrix}X_1 X_2 \cdots X_p \end{pmatrix}$ be our data matrix of size $n \times p$, where $n$ is the number of observations and $p$ the number of variables. Construct matrix $\mathbf{D}$ by centering and normalizing the matrix $\mathbf{X}$:
\begin{equation}
	\mathbf{D} = \begin{pmatrix}
			\frac{X_1 - \bar{X}_1}{\lVert X_1 \rVert} \frac{X_2 - \bar{X}_2}{\lVert X_2 \rVert} \cdots \frac{X_p - \bar{X}_p}{\lVert X_p \rVert} 
		\end{pmatrix}.
\end{equation}
Construct the $p \times p$ correlation matrix $\mathbf{C} = \mathbf{D}^T\mathbf{D} / (n - 1)$. 
Performing the singular value decomposition on the matrix $\mathbf{C}$ results in:
\begin{equation}
	\mathbf{D} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^T,
\end{equation}
where $\mathbf{U}$ is a unitary matrix, $\mathbf{\Sigma}$ is a diagonal matrix of singular values $s_i$, and $\mathbf{V}$ is a matrix of eigenvectors. 
The relationship between the singular values and the eigenvalues of the correlation matrix is the following:
\begin{equation}
\begin{aligned}
	\mathbf{C} &= \mathbf{D}^T\mathbf{D} / (n - 1) \\
	&= (\mathbf{U}\mathbf{\Sigma}\mathbf{V}^T)^T\mathbf{U}\mathbf{\Sigma}\mathbf{V}^T / (n - 1) \\
	&= \mathbf{V}\mathbf{\Sigma}\mathbf{U}^T\mathbf{U}\mathbf{\Sigma}\mathbf{V}^T / (n - 1) \\
	&= \mathbf{V}\mathbf{\Sigma}^2\mathbf{V}^T / (n - 1).
\end{aligned}
\end{equation}
From this, we can conclude that singular values are related to eigenvalues in the following way: $\lambda_i = s_i^2 / (n-1)$. Principal components can be obtain using: $\mathbf{D}\mathbf{V} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^T\mathbf{V} = \mathbf{U}\mathbf{\Sigma}$.

\subsection{Sparse Principal Component Analysis (SPCA)}
\label{sec:spca}
Before we get to the definition of sparse principal component analysis, we first have to look at some regression regularization techniques. 
\textcite{bell_solutions_1978} introduced the ridge regression.
The ridge regression is a regularization method to mitigate the problem of multicollinearity in linear regression.
It imposes a penalty on the squared value of the regression coefficients, effectively shrinking them towards zero.
\begin{equation}
	\beta_{ridge} = \argmin_{\beta} \left\{ { \left\lVert Y - \sum_{j=1}^p X_j \beta_j \right\rVert }^2 + \lambda \sum_{j=1}^p \beta_j^2 \right\} 
\end{equation}
\textcite{tibshirani_regression_1996} later introduced the \textit{least absolute shrinkage and selection operator} (lasso) regression. 
This method imposes a penalty on the absolute value of the regression coefficients, effectively eliminating some coefficients from the model.
\begin{equation}
	\beta_{lasso} = \argmin_{\beta} \left\{ { \left\lVert Y - \sum_{j=1}^p X_j \beta_j \right\rVert }^2 + \lambda \sum_{j=1}^p |\beta_j| \right\} 
\end{equation}
Combining these two regularization methods resulted in what is now known as the elastic net, see~\textcite{zou_regularization_2005}.
\begin{equation}
	\beta_{elastic\ net} = (1 + \lambda_2) \argmin_{\beta} \left\{ { \left\lVert Y - \sum_{j=1}^p X_j \beta_j \right\rVert }^2 + \lambda_2 \sum_{j=1}^p \beta_j^2 + \lambda_1 \sum_{j=1}^p |\beta_j|  \right\} 
\end{equation}
Factors estimated with Principal Component Analysis create linear combinations of the underlying predictor variables with all non-zero factor loadings. 
This negatively affects the parsimony of forecasting models and makes interpreting the factor loadings difficult. 
To overcome this, \textcite{zou_sparse_2006} introduced a method of finding principal components with sparse factor loadings by formulating PCA as a regression problem and applying the elastic net. 
\begin{equation}
\begin{aligned}
	\left(\mathbf{\widehat{A}},\mathbf{\widehat{B}}\right) = \argmin_{\mathbf{A},\mathbf{B}}\ &\sum_{i=1}^n { \left\lVert \mathbf{x}_i - \mathbf{A}\mathbf{B}^T\mathbf{x}_i \right\rVert }^2 + \lambda_2 \sum_{j=1}^k {\lVert \beta_j \rVert}^2 + \sum_{j=1}^k \lambda_{1,j} {\lVert \beta_j \rVert}_1 \\
	\text{subject to}\ &\mathbf{A}^T\mathbf{A} = \mathbf{I}_{k \times k}
\end{aligned}
\end{equation}
One can obtain the factors as columns, after normalization, of the matrix $\mathbf{\widehat{B}}$.

\section{Models}
\label{sec:models}
For a summary of the models used, see~\cref{tab:model-description}. 
The details of each model is given below.
Each section contains a short list of the corresponding models from~\cref{tab:model-description}.
\subsection{Autoregressive (AR) and Vector-Autoregressive (VAR) Models}
\label{sec:arvar}
\textit{Models in this section are summarized by: AR(1), VAR(1), VAR(1)+FB1(PCA), VAR(1)+FB2(PCA), VAR(1)+FB1(SPCA), VAR(1)+FB2(SPCA), AR(SIC), VAR(SIC), VAR(SIC)+FB1(PCA), VAR(SIC)+FB2(PCA), VAR(SIC)+FB1(SPCA), and VAR(SIC)+FB2(SPCA). See~\cref{tab:model-description}, panel A.}

These AR($p$) and VAR($p$) models are formulated as follows:
\begin{equation}
	\hat{y}_{t+1}(\tau) = \hat{c} + \hat{\beta}' W_t,
\end{equation}
where $\tau$ denotes the maturity and $\hat{y}_{t+1}(\tau)$ measures the 1-step ahead annual yield forecast of the bond. 
For the autoregressive model, $W_t$ contains $p$ lags of $y_{t+1}(\tau)$. For the vector autoregressive model, $W_t$ additionally contains yields of bonds of different maturities than $\tau$. 
In both models $\hat{\beta}$ is a time-invariant coefficient vector and $\hat{c}$ is a (vector of) constant(s). 
We will be including AR(1) and VAR(1) models as a baseline. 
Furthermore, we will be including AR and VAR specifications with up to 5 lags of $y_{t+1}(\tau)$ included. 
The number of lags shall be selected using the Schwarz information criterion.
The Schwarz information criterion, also commonly referred to as the Bayesian information criterion, published by~\textcite[hereafter SIC]{schwarz_estimating_1978} is a model selection criterium, closely related to the Akaike information criterion~\parencite[hereafter AIC]{akaike_new_1974}. 
It is defined as follows:
\begin{equation}
	SIC = k\ln{n} - 2\ln{\hat{L}},
\end{equation}
where $\hat{L}$ is the likelihood of the model, $n$ the number of observations and $k$ the number of model parameters estimated. The difference between SIC and AIC is the higher penalty given for the addition of extra parameters to compensate for the increase in likelihood.

\subsection{Dynamic Nelson-Siegel (DNS) Models}
\label{sec:dns}
\textit{Models in this section are summarized by: DNS(1-6), DNS(1-6)+FB1(PCA), DNS(1-6)+FB2(PCA), DNS(1-6)+FB1(SPCA), DNS(1-6)+FB2(SPCA), and DNS(1-6)+MAC. See~\cref{tab:model-description}, panel B.}

The Nelson-Siegel model, published by~\textcite{nelson_parsimonious_1987}, models the cross-sectional movement of the term structure using three underlying latent factors interpreted as \enquote{level}, \enquote{slope}, and \enquote{curvature}. 
These factors are commonly known as the \enquote{Nelson-Siegel factors}. 
The dynamic version of the Nelson-Siegel model was published by~\textcite[hereafter DNS]{diebold_forecasting_2006}. 
The DNS model is formulated as follows:
\begin{equation}
	\hat{y}_{t+1}(\tau) = \hat{\beta}_{1,t+1}^{f} + \hat{\beta}_{2,t+1}^{f} \left[\frac{1-\exp(-\lambda_t \tau)}{\lambda_t \tau}\right] + \hat{\beta}_{3,t+1}^{f} \left[\frac{1-\exp(-\lambda_t \tau)}{\lambda_t \tau} - \exp(-\lambda_t \tau)\right].
\end{equation}
To estimate the DNS model, we make use of the two-step formulation, presented by~\textcite{diebold_forecasting_2006}. 
The first step is obtaining the parameters $\hat{\beta}_{1,t}^f, \hat{\beta}_{2,t}^f, \hat{\beta}_{3,t}^f$ by regressing $$\left\{1, \left[\frac{1-\exp(-\lambda_t \tau)}{\lambda_t \tau}\right], \left[\frac{1-\exp(-\lambda_t \tau)}{\lambda_t \tau} - \exp(-\lambda_t \tau)\right] \right\}$$ on $\mathbf{y}_t^f(\tau)$.
We consider 3 variants of $\mathbf{y}_t^f(\tau)$ in this paper, namely:
\begin{align*}
	\mathbf{y}_t^{10}(\tau) &= \left[y_t(12)~y_t(24)~y_t(36)~y_t(48)~y_t(60)~y_t(72)~y_t(84)~y_t(96)~y_t(108)~y_t(120) \right]', \\
	\mathbf{y}_t^6(\tau) &= \left[y_t(12)~y_t(24)~y_t(36)~y_t(60)~y_t(84)~y_t(120) \right]', \\
	\mathbf{y}_t^4(\tau) &= \left[y_t(12)~y_t(36)~y_t(60)~y_t(120) \right]'.
\end{align*}
\textcite{diebold_forecasting_2006} fix $\lambda_t$ to $0.0609$.
This maximizes the loading on the curvature factor at a 30 month maturity. 
The second step is estimating $\hat{\beta}_{1,t+1}^{f}$, $\hat{\beta}_{2,t+1}^{f}$, and $\hat{\beta}_{3,t+1}^{f}$.
We estimate these parameters using both an AR(1) and a VAR(1) model. 
The AR(1) model is formulated as follows:
\begin{equation}
\label{eq:dnsar}
	\hat{\beta}_{i,t+1}^{f} = \hat{c}_i + \hat{\gamma}_{ii} \hat{\beta}_{i,t}^f, \quad \text{for}~i = 1,2,3,
\end{equation}
where $\hat{c}_i$, $\hat{\gamma}_{ii}$, and $\hat{\beta}_{i,t}^f$ are scalars. 
The VAR(1) model is formulated as follows:
\begin{equation}
\label{eq:dnsvar}
	\hat{\beta}_{t+1}^{f} = \hat{c} + \hat{\Gamma} \hat{\beta}_t^f,
\end{equation}
where $\hat{c}$ is a $3 \times 1$ vector, $\hat{\beta}_t^f = \left(\hat{\beta}_{1,t}^f, \hat{\beta}_{2,t}^f, \hat{\beta}_{3,t}^f\right)'$ a $3 \times 1$ vector, and $\hat{\Gamma} = \left(\hat{\gamma}_1, \hat{\gamma}_2, \hat{\gamma}_3 \right)$, with $\hat{\gamma}_j$ a $3 \times 1$ vector, for $j = 1,2,3$. 

\subsubsection{DNS Models with Macroeconomic Variables}
\label{sec:dnsmv}
The DNS model, described in~\cref{sec:dns}, can be extended by including macroeconomic variables in the AR(1) model, given in~\cref{eq:dnsar}, as follows:
\begin{equation}
	\hat{\beta}_{i,t+1}^{f} = \hat{c}_i + \hat{\gamma}_{ii} \hat{\beta}_{i,t}^f + \hat{\alpha}_i' M_t, \quad \text{for}~i = 1,2,3,
\end{equation}
where $\hat{\alpha}_i$ is a $3 \times 1$ vector, and $M_t$ is a $3 \times 1$ vector containing the macroeconomic variables \enquote{manufacturing capacity utilization}, \enquote{the federal funds rate}, and \enquote{the annual personal consumption expenditures price deflator}. 
All other terms are conformably defined. 
Analogous to the AR(1) model, the VAR(1) given in~\cref{eq:dnsvar}, can also be extended by including macroeconomic variables:
\begin{equation}
	\hat{\beta}_{t+1}^{f} = \hat{c} + \hat{\Gamma} \hat{\beta}_t^f + \hat{\mathrm{A}} M_t,
\end{equation}
where $\hat{\mathrm{A}} = \left(\hat{\alpha}_1, \hat{\alpha}_2, \hat{\alpha}_3 \right)$, with $\hat{\alpha}_j$ a $3 \times 1$ vector, for $j = 1,2,3$. 
All other terms are conformably defined.

\subsubsection{DNS Models with Diffusion Indexes}
\label{sec:dnsdif}
The DNS model given in~\cref{sec:dns}, can also be extended by including diffusion indexes. The extended version of the AR(1) model is formulated as follows:
\begin{equation}
	\hat{\beta}_{i,t+1}^{f} = \hat{c}_i + \hat{\gamma}_{ii} \hat{\beta}_{i,t}^f + \hat{\xi}' F_t^b, \quad \text{for}~i = 1,2,3,
\end{equation}
where $F_t^b$ is defined as in~\cref{sec:dif} and $\hat{\xi}$ is a conformably sized (depending on the amount of factors included) vector of coefficients. 
All other terms are conformably defined. 
Analogous to the AR(1) model, the VAR(1) given in~\cref{eq:dnsvar}, can also be extended by including diffusion indexes:
\begin{equation}
	\hat{\beta}_{t+1}^{f} = \hat{c} + \hat{\Gamma} \hat{\beta}_t^f + \hat{\Xi} F_t^b,
\end{equation}
where $\hat{\Xi}$ is a conformably sized (depending on the amount of factors included) matrix of coefficients. 
All other terms are conformably defined.

\subsection{Diffusion Index Models}
\label{sec:dif}
\textit{Models in this section are summarized by: DIF(1-9), DIF(1-3)+FB1(PCA), DIF(1-3)+FB2(PCA), DIF(1-3)+FB1(SPCA), and DIF(1-3)+FB2(SPCA). See~\cref{tab:model-description}, panel C.}

Diffusion Index Models, also commonly referred to as \enquote{factor augmented forecast models}, as published by~\textcite[hereafter DIF]{stock_macroeconomic_2002,stock_forecasting_2002}, is an extension to regular forecast models, mostly (vector-)autoregressive models, supplemented by a vector of latent factors. 
This model is formulated as follows:
\begin{equation}
	\hat{y}_{t+1}(\tau) = \hat{c} + \hat{\beta}' W_t + \hat{\xi}' F_t^b + \hat{\eta}' F_t^s,
\end{equation}
where $F_t^b$ contains either 1, 2, or 3 latent factors, estimated from the set of macroeconomic variables, using either PCA or SPCA (see~\cref{sec:spca}), $F_t^s$ contains either 1, 2, or 3 latent factors, estimated with PCA, from a set of 10 yields given by $\mathbf{y}_t^{10}(\tau)$, see~\cref{sec:dns}, and $\hat{\xi}$ and $\hat{\eta}$ are conformably sized (depending on the amount of factors included) vectors of coefficients. 

\section{Model evaluation}
\subsection{Mean Squared Prediction Error (MSPE)}
\label{sec:mspe}
The Mean Square Prediction Error, hereafter MSPE, is a metric for evaluating models in forecasting.
It is defined as follows:
\begin{equation}
	MSPE = \frac{1}{q} \sum_{i=n+1}^{n+q} \left(Y_i - \hat{Y}_i \right)^2,
\end{equation}
where $n$ is the training sample and $q$ is the forecasting sample.

\subsection{Diebold-Mariano (DM) Test}
\label{sec:dmtest}
The Diebold-Mariano Test, published by~\textcite[hereafter DM]{diebold_comparing_1994}, is a predictive accuracy test between two models. 
The hypotheses are defined as follows:
\begin{align*}
	H_0 : E\left[u_{0,t+h}^2 - u_{1,t+h}^2\right] = 0, \\
	H_A : E\left[u_{0,t+h}^2 - u_{1,t+h}^2\right] \neq 0,
\end{align*}
where $u_{0,t+h}$ and $u_{1,t+h}$ are $h$-step ahead forecast errors. 
In practice, we only observe estimates of these out-of-sample $h$-step ahead forecasts, i.e. $\hat{u}_{0,t+h}$ and $\hat{u}_{1,t+h}$. 
The standard version of the DM predictive accuracy test is formulated as follows:
\begin{equation}
	DM_p = \frac{\bar{d_t}}{\hat{\sigma}_{\bar{d}_t}} \overset{d}{\rightarrow} N(0,1),
\end{equation}
where
\begin{equation}
	\bar{d}_t = \frac{1}{q} \sum_{t=n+1}^{n+q} d_t,\ d_t = \hat{u}_{0,t+h}^2 - \hat{u}_{1,t+h}^2,\ \text{and}\ \hat{\sigma}_{\bar{d}_t} = \frac{\hat{\sigma}_{d_t}}{\sqrt{q}}.
\end{equation}
In the formulation above, $q$ is the number of forecasts, $n$ is the number of observations in the training sample, and $\hat{\sigma}_{d_t}$ is the standard deviation of the forecast errors.
